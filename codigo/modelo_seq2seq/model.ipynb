{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3610f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3abce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../datos\"\n",
    "supplemental = pd.read_csv(os.path.join(data_path, \"supplemental_clinical_data.csv\"))\n",
    "patient =pd.read_csv(os.path.join(data_path, \"train_clinical_data.csv\"))\n",
    "peptides = pd.read_csv(os.path.join(data_path, \"train_peptides.csv\"))\n",
    "proteins = pd.read_csv(os.path.join(data_path, \"train_proteins.csv\"))\n",
    "\n",
    "supplemental.loc[supplemental[\"visit_month\"] == 5, \"visit_month\"] = 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2bcc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_patient = pd.concat([patient,supplemental])\n",
    "updrs_ranges = [52,52,132,24]\n",
    "updrs_cols = [f\"updrs_{i}\" for i in range(1,5)]\n",
    "for updrs_range, col in zip(updrs_ranges, updrs_cols):\n",
    "    scaled_patient[col] /= updrs_range\n",
    "\n",
    "scaled_protein = proteins.copy()\n",
    "scaled_protein[\"NPX\"] = np.log2(proteins[\"NPX\"])\n",
    "scaled_protein = (\n",
    "    scaled_protein[[\"UniProt\", \"NPX\"]]\n",
    "    .groupby(\"UniProt\")\n",
    "    .agg([\"min\", \"max\"])\n",
    "    .droplevel(0, axis=1)\n",
    "    .join(proteins.set_index(\"UniProt\"))\n",
    ")\n",
    "scaled_protein[\"NPX\"] = (scaled_protein[\"NPX\"] - scaled_protein[\"min\"]) / (\n",
    "    scaled_protein[\"max\"] - scaled_protein[\"min\"]\n",
    ").drop(columns=[\"min\", \"max\"])\n",
    "\n",
    "scaled_peptide = peptides.copy()\n",
    "scaled_peptide[\"PeptideAbundance\"]= np.log2(peptides[\"PeptideAbundance\"])\n",
    "scaled_peptide = (\n",
    "    scaled_peptide[[\"UniProt\", \"PeptideAbundance\", \"Peptide\"]]\n",
    "    .groupby([\"UniProt\", \"Peptide\"])\n",
    "    .agg([\"min\", \"max\"])\n",
    "    .droplevel(0, axis=1)\n",
    "    .join(peptides.set_index([\"UniProt\", \"Peptide\"]))\n",
    ")\n",
    "scaled_peptide[\"PeptideAbundance\"] = (scaled_peptide[\"PeptideAbundance\"] - scaled_peptide[\"min\"]) / (\n",
    "    scaled_peptide[\"max\"] - scaled_peptide[\"min\"]\n",
    ").drop(columns=[\"min\", \"max\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07bcca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaled_patient = scaled_patient.rename(\n",
    "    columns={\"upd23b_clinical_state_on_medication\": \"on_medication\"}\n",
    ")\n",
    "scaled_patient[\"on_medication\"] = (\n",
    "    scaled_patient[\"on_medication\"]\n",
    "    .case_when(\n",
    "        [\n",
    "            (scaled_patient.on_medication.eq(\"On\"), 1),\n",
    "            (scaled_patient.on_medication.eq(\"Off\"), -1),\n",
    "        ]\n",
    "    )\n",
    "    .fillna(\"0\")\n",
    ")\n",
    "\n",
    "scaled_patient = (\n",
    "    scaled_patient.set_index([\"patient_id\", \"visit_month\"])\n",
    "    .join(\n",
    "        scaled_peptide.pivot_table(\n",
    "            values=\"PeptideAbundance\",\n",
    "            index=[\"patient_id\", \"visit_month\"],\n",
    "            columns=[\"Peptide\"],\n",
    "            aggfunc=\"sum\",\n",
    "        ).fillna(0)\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "scaled_patient = (\n",
    "    scaled_patient.set_index([\"patient_id\", \"visit_month\"])\n",
    "    .join(\n",
    "        scaled_protein.pivot_table(\n",
    "            values=\"NPX\",\n",
    "            index=[\"patient_id\", \"visit_month\"],\n",
    "            columns=[\"UniProt\"],\n",
    "            aggfunc=\"sum\",\n",
    "        ).fillna(0)\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "scaled_patient = scaled_patient.fillna(0)\n",
    "# scale visit month\n",
    "max_month = scaled_patient.visit_month.max() # keep max to deescale later\n",
    "scaled_patient[\"visit_month\"] = scaled_patient[\"visit_month\"] / max_month\n",
    "protein_cols = list(scaled_protein.index.unique())\n",
    "peptide_cols = list(scaled_peptide.index.unique().to_series().apply(lambda t: t[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ff3c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./codigo/modelo_seq2seq\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "360c45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from format_seqs import format_data\n",
    "from aux import train_val_split\n",
    "\n",
    "input_feautures_name = \"BASE\"\n",
    "\n",
    "base_encoder_input_features = [\n",
    "    \"visit_month\",\n",
    "    \"updrs_1\",\n",
    "    \"updrs_2\",\n",
    "    \"updrs_3\",\n",
    "    \"updrs_4\",\n",
    "]\n",
    "protein_encoder_input_features = base_encoder_input_features + protein_cols\n",
    "peptide_encoder_input_features = base_encoder_input_features + peptide_cols\n",
    "\n",
    "encoder_input_features = (protein_encoder_input_features if input_feautures_name == \"PROTEIN\" else peptide_encoder_input_features if input_feautures_name == \"PEPTIDE\" else base_encoder_input_features\n",
    "                          )\n",
    "decoder_input_features = [\n",
    "    \"visit_month\",\n",
    "    \"updrs_1\",\n",
    "    \"updrs_2\",\n",
    "    \"updrs_3\",\n",
    "    \"updrs_4\",\n",
    "]\n",
    "output_features = decoder_input_features[1:]  # visit_month is the only covariable\n",
    "\n",
    "data, target_indices = format_data(\n",
    "    scaled_patient,\n",
    "    partition_key=\"patient_id\",\n",
    "    order_key=\"visit_month\",\n",
    "    encoder_input_features=encoder_input_features,\n",
    "    decoder_input_features=decoder_input_features,\n",
    "    output_features=output_features,\n",
    "    input_seq_length=3,\n",
    "    output_seq_length=3,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a091bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import Encoder as LSTMEncoder, DecoderWithAttention as LSTMDecoder\n",
    "from seq2seq import Encoder as GRUEncoder, DecoderWithAttention as GRUDecoder, Seq2Seq\n",
    "rnn_type = \"LSTM\"\n",
    "if rnn_type == \"LSTM\":\n",
    "    Encoder, DecoderWithAttention = LSTMEncoder, LSTMDecoder\n",
    "else:\n",
    "    Encoder, DecoderWithAttention = GRUEncoder, GRUDecoder\n",
    "\n",
    "enc_feature_size = len(encoder_input_features)\n",
    "hidden_size = 16\n",
    "num_layers = 1\n",
    "dropout = 0.1\n",
    "dec_feature_size = len(decoder_input_features)\n",
    "dec_target_size = len(output_features)\n",
    "device = 'cpu'\n",
    "lr = 0.00025\n",
    "grad_clip = 1\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "decay = 5 # Lower means faster decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6372ff83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 => Train loss: 159.86044, Val: 158.64383, Teach: 0.83, Took 0.1 s      (NEW BEST)\n",
      "Epoch 2 => Train loss: 152.56536, Val: 149.88038, Teach: 0.80, Took 0.1 s      (NEW BEST)\n",
      "Epoch 3 => Train loss: 139.80156, Val: 138.83539, Teach: 0.77, Took 0.1 s      (NEW BEST)\n",
      "Epoch 4 => Train loss: 129.58438, Val: 130.32601, Teach: 0.73, Took 0.1 s      (NEW BEST)\n",
      "Epoch 5 => Train loss: 122.71402, Val: 123.98438, Teach: 0.69, Took 0.1 s      (NEW BEST)\n",
      "Epoch 6 => Train loss: 120.68081, Val: 123.25453, Teach: 0.65, Took 0.1 s      (NEW BEST)\n",
      "Epoch 7 => Train loss: 120.19856, Val: 122.98842, Teach: 0.60, Took 0.1 s      (NEW BEST)\n",
      "Epoch 8 => Train loss: 119.32075, Val: 118.44588, Teach: 0.55, Took 0.1 s      (NEW BEST)\n",
      "Epoch 9 => Train loss: 114.70169, Val: 104.08895, Teach: 0.50, Took 0.1 s      (NEW BEST)\n",
      "Epoch 10 => Train loss: 99.76326, Val: 90.04103, Teach: 0.45, Took 0.1 s      (NEW BEST)\n",
      "Epoch 11 => Train loss: 89.39515, Val: 86.00287, Teach: 0.40, Took 0.1 s      (NEW BEST)\n",
      "Epoch 12 => Train loss: 84.90252, Val: 86.03259, Teach: 0.36, Took 0.1 s\n",
      "Epoch 13 => Train loss: 83.62872, Val: 84.61463, Teach: 0.31, Took 0.1 s      (NEW BEST)\n",
      "Epoch 14 => Train loss: 83.13263, Val: 83.76488, Teach: 0.27, Took 0.1 s      (NEW BEST)\n",
      "Epoch 15 => Train loss: 82.19548, Val: 83.02600, Teach: 0.23, Took 0.1 s      (NEW BEST)\n",
      "Epoch 16 => Train loss: 81.77399, Val: 83.04943, Teach: 0.20, Took 0.1 s\n",
      "Epoch 17 => Train loss: 81.27927, Val: 81.40035, Teach: 0.17, Took 0.1 s      (NEW BEST)\n",
      "Epoch 18 => Train loss: 81.13615, Val: 81.70589, Teach: 0.14, Took 0.1 s\n",
      "Epoch 19 => Train loss: 80.29598, Val: 81.39520, Teach: 0.12, Took 0.1 s      (NEW BEST)\n",
      "Epoch 20 => Train loss: 80.32531, Val: 81.15348, Teach: 0.10, Took 0.1 s      (NEW BEST)\n",
      "Epoch 21 => Train loss: 79.63240, Val: 80.17465, Teach: 0.08, Took 0.1 s      (NEW BEST)\n",
      "Epoch 22 => Train loss: 79.02366, Val: 80.59304, Teach: 0.07, Took 0.1 s\n",
      "Epoch 23 => Train loss: 78.05613, Val: 78.92659, Teach: 0.06, Took 0.1 s      (NEW BEST)\n",
      "Epoch 24 => Train loss: 77.93145, Val: 80.01168, Teach: 0.05, Took 0.1 s\n",
      "Epoch 25 => Train loss: 78.37835, Val: 79.31218, Teach: 0.04, Took 0.1 s\n",
      "Epoch 26 => Train loss: 78.74130, Val: 80.43970, Teach: 0.03, Took 0.1 s\n",
      "Epoch 27 => Train loss: 77.23245, Val: 78.32716, Teach: 0.03, Took 0.1 s      (NEW BEST)\n",
      "Epoch 28 => Train loss: 77.07540, Val: 78.42038, Teach: 0.02, Took 0.1 s\n",
      "Epoch 29 => Train loss: 78.06286, Val: 78.75991, Teach: 0.02, Took 0.1 s\n",
      "Epoch 30 => Train loss: 76.26427, Val: 78.73601, Teach: 0.01, Took 0.1 s\n",
      "Epoch 31 => Train loss: 76.18909, Val: 79.55380, Teach: 0.01, Took 0.1 s\n",
      "Epoch 32 => Train loss: 76.46553, Val: 78.26883, Teach: 0.01, Took 0.1 s      (NEW BEST)\n",
      "Epoch 33 => Train loss: 76.14239, Val: 77.39148, Teach: 0.01, Took 0.1 s      (NEW BEST)\n",
      "Epoch 34 => Train loss: 76.54572, Val: 77.47958, Teach: 0.01, Took 0.1 s\n",
      "Epoch 35 => Train loss: 75.71570, Val: 77.90579, Teach: 0.01, Took 0.1 s\n",
      "Epoch 36 => Train loss: 75.84065, Val: 76.56801, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 37 => Train loss: 76.22441, Val: 78.18220, Teach: 0.00, Took 0.1 s\n",
      "Epoch 38 => Train loss: 76.09166, Val: 76.96102, Teach: 0.00, Took 0.1 s\n",
      "Epoch 39 => Train loss: 75.03517, Val: 77.29175, Teach: 0.00, Took 0.1 s\n",
      "Epoch 40 => Train loss: 74.96335, Val: 76.70441, Teach: 0.00, Took 0.1 s\n",
      "Epoch 41 => Train loss: 75.28315, Val: 77.56520, Teach: 0.00, Took 0.1 s\n",
      "Epoch 42 => Train loss: 76.47966, Val: 76.75812, Teach: 0.00, Took 0.1 s\n",
      "Epoch 43 => Train loss: 75.26177, Val: 76.65827, Teach: 0.00, Took 0.1 s\n",
      "Epoch 44 => Train loss: 75.00193, Val: 76.04885, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 45 => Train loss: 75.35423, Val: 76.64661, Teach: 0.00, Took 0.1 s\n",
      "Epoch 46 => Train loss: 75.67280, Val: 77.13150, Teach: 0.00, Took 0.1 s\n",
      "Epoch 47 => Train loss: 74.99004, Val: 77.46286, Teach: 0.00, Took 0.1 s\n",
      "Epoch 48 => Train loss: 74.00313, Val: 76.17970, Teach: 0.00, Took 0.1 s\n",
      "Epoch 49 => Train loss: 75.18314, Val: 76.48697, Teach: 0.00, Took 0.1 s\n",
      "Epoch 50 => Train loss: 74.85781, Val: 76.57063, Teach: 0.00, Took 0.1 s\n",
      "Epoch 51 => Train loss: 74.63581, Val: 76.00146, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 52 => Train loss: 73.73103, Val: 76.49979, Teach: 0.00, Took 0.1 s\n",
      "Epoch 53 => Train loss: 74.04410, Val: 75.86825, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 54 => Train loss: 74.33007, Val: 76.44150, Teach: 0.00, Took 0.1 s\n",
      "Epoch 55 => Train loss: 74.66750, Val: 75.83508, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 56 => Train loss: 74.06505, Val: 75.64908, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 57 => Train loss: 74.15900, Val: 75.32124, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 58 => Train loss: 74.09868, Val: 75.14373, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 59 => Train loss: 73.48526, Val: 74.92158, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 60 => Train loss: 73.36888, Val: 75.36015, Teach: 0.00, Took 0.1 s\n",
      "Epoch 61 => Train loss: 73.88060, Val: 75.66601, Teach: 0.00, Took 0.1 s\n",
      "Epoch 62 => Train loss: 74.04311, Val: 75.35950, Teach: 0.00, Took 0.1 s\n",
      "Epoch 63 => Train loss: 73.66956, Val: 74.37162, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 64 => Train loss: 73.82911, Val: 75.22924, Teach: 0.00, Took 0.1 s\n",
      "Epoch 65 => Train loss: 73.65677, Val: 75.81035, Teach: 0.00, Took 0.1 s\n",
      "Epoch 66 => Train loss: 73.16075, Val: 75.15191, Teach: 0.00, Took 0.1 s\n",
      "Epoch 67 => Train loss: 74.07515, Val: 73.84795, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 68 => Train loss: 73.12091, Val: 75.17224, Teach: 0.00, Took 0.1 s\n",
      "Epoch 69 => Train loss: 73.87324, Val: 74.56264, Teach: 0.00, Took 0.1 s\n",
      "Epoch 70 => Train loss: 73.29040, Val: 74.28676, Teach: 0.00, Took 0.1 s\n",
      "Epoch 71 => Train loss: 72.96603, Val: 76.07358, Teach: 0.00, Took 0.1 s\n",
      "Epoch 72 => Train loss: 73.17189, Val: 74.95166, Teach: 0.00, Took 0.1 s\n",
      "Epoch 73 => Train loss: 73.98557, Val: 75.78010, Teach: 0.00, Took 0.1 s\n",
      "Epoch 74 => Train loss: 72.86462, Val: 74.87609, Teach: 0.00, Took 0.1 s\n",
      "Epoch 75 => Train loss: 73.70096, Val: 75.32374, Teach: 0.00, Took 0.1 s\n",
      "Epoch 76 => Train loss: 73.49368, Val: 74.82483, Teach: 0.00, Took 0.1 s\n",
      "Epoch 77 => Train loss: 73.45276, Val: 74.88305, Teach: 0.00, Took 0.1 s\n",
      "Epoch 78 => Train loss: 73.72700, Val: 74.99249, Teach: 0.00, Took 0.1 s\n",
      "Epoch 79 => Train loss: 73.12071, Val: 74.69447, Teach: 0.00, Took 0.1 s\n",
      "Epoch 80 => Train loss: 73.42758, Val: 74.55153, Teach: 0.00, Took 0.1 s\n",
      "Epoch 81 => Train loss: 73.10605, Val: 74.67403, Teach: 0.00, Took 0.1 s\n",
      "Epoch 82 => Train loss: 73.65734, Val: 74.86670, Teach: 0.00, Took 0.1 s\n",
      "Epoch 83 => Train loss: 73.22529, Val: 75.09508, Teach: 0.00, Took 0.1 s\n",
      "Epoch 84 => Train loss: 72.89787, Val: 74.20646, Teach: 0.00, Took 0.1 s\n",
      "Epoch 85 => Train loss: 72.70778, Val: 74.54509, Teach: 0.00, Took 0.1 s\n",
      "Epoch 86 => Train loss: 73.04097, Val: 74.18514, Teach: 0.00, Took 0.1 s\n",
      "Epoch 87 => Train loss: 73.48427, Val: 73.54936, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 88 => Train loss: 73.44359, Val: 75.04892, Teach: 0.00, Took 0.1 s\n",
      "Epoch 89 => Train loss: 73.41267, Val: 74.34596, Teach: 0.00, Took 0.1 s\n",
      "Epoch 90 => Train loss: 72.92785, Val: 74.91946, Teach: 0.00, Took 0.1 s\n",
      "Epoch 91 => Train loss: 72.81135, Val: 75.91195, Teach: 0.00, Took 0.1 s\n",
      "Epoch 92 => Train loss: 73.60679, Val: 74.22556, Teach: 0.00, Took 0.1 s\n",
      "Epoch 93 => Train loss: 72.76873, Val: 74.46287, Teach: 0.00, Took 0.1 s\n",
      "Epoch 94 => Train loss: 72.57924, Val: 75.08897, Teach: 0.00, Took 0.1 s\n",
      "Epoch 95 => Train loss: 72.67414, Val: 73.62154, Teach: 0.00, Took 0.1 s\n",
      "Epoch 96 => Train loss: 72.50878, Val: 74.66101, Teach: 0.00, Took 0.1 s\n",
      "Epoch 97 => Train loss: 73.32852, Val: 74.43089, Teach: 0.00, Took 0.1 s\n",
      "Epoch 98 => Train loss: 73.09029, Val: 75.29709, Teach: 0.00, Took 0.1 s\n",
      "Epoch 99 => Train loss: 72.51800, Val: 74.74931, Teach: 0.00, Took 0.1 s\n",
      "Epoch 100 => Train loss: 72.62518, Val: 75.39502, Teach: 0.00, Took 0.1 s\n",
      "Epoch 1 => Train loss: 164.17129, Val: 145.22005, Teach: 0.83, Took 0.1 s      (NEW BEST)\n",
      "Epoch 2 => Train loss: 149.47143, Val: 137.92235, Teach: 0.80, Took 0.1 s      (NEW BEST)\n",
      "Epoch 3 => Train loss: 138.15227, Val: 136.64057, Teach: 0.77, Took 0.1 s      (NEW BEST)\n",
      "Epoch 4 => Train loss: 132.16842, Val: 133.11769, Teach: 0.73, Took 0.1 s      (NEW BEST)\n",
      "Epoch 5 => Train loss: 127.87863, Val: 122.93018, Teach: 0.69, Took 0.1 s      (NEW BEST)\n",
      "Epoch 6 => Train loss: 117.74548, Val: 107.49161, Teach: 0.65, Took 0.1 s      (NEW BEST)\n",
      "Epoch 7 => Train loss: 105.15191, Val: 98.16706, Teach: 0.60, Took 0.1 s      (NEW BEST)\n",
      "Epoch 8 => Train loss: 94.13824, Val: 92.51822, Teach: 0.55, Took 0.1 s      (NEW BEST)\n",
      "Epoch 9 => Train loss: 89.52915, Val: 91.28060, Teach: 0.50, Took 0.1 s      (NEW BEST)\n",
      "Epoch 10 => Train loss: 87.05946, Val: 90.26840, Teach: 0.45, Took 0.1 s      (NEW BEST)\n",
      "Epoch 11 => Train loss: 85.71489, Val: 90.26980, Teach: 0.40, Took 0.1 s\n",
      "Epoch 12 => Train loss: 85.23376, Val: 88.15091, Teach: 0.36, Took 0.1 s      (NEW BEST)\n",
      "Epoch 13 => Train loss: 85.61059, Val: 88.21941, Teach: 0.31, Took 0.1 s\n",
      "Epoch 14 => Train loss: 84.37101, Val: 87.24878, Teach: 0.27, Took 0.1 s      (NEW BEST)\n",
      "Epoch 15 => Train loss: 83.21359, Val: 86.00269, Teach: 0.23, Took 0.1 s      (NEW BEST)\n",
      "Epoch 16 => Train loss: 83.16769, Val: 85.82705, Teach: 0.20, Took 0.1 s      (NEW BEST)\n",
      "Epoch 17 => Train loss: 81.75983, Val: 85.22355, Teach: 0.17, Took 0.1 s      (NEW BEST)\n",
      "Epoch 18 => Train loss: 81.03069, Val: 84.41552, Teach: 0.14, Took 0.1 s      (NEW BEST)\n",
      "Epoch 19 => Train loss: 79.72744, Val: 83.31330, Teach: 0.12, Took 0.1 s      (NEW BEST)\n",
      "Epoch 20 => Train loss: 80.14476, Val: 83.71024, Teach: 0.10, Took 0.1 s\n",
      "Epoch 21 => Train loss: 79.26925, Val: 84.17287, Teach: 0.08, Took 0.1 s\n",
      "Epoch 22 => Train loss: 78.46477, Val: 83.27961, Teach: 0.07, Took 0.1 s      (NEW BEST)\n",
      "Epoch 23 => Train loss: 78.82755, Val: 82.03335, Teach: 0.06, Took 0.1 s      (NEW BEST)\n",
      "Epoch 24 => Train loss: 77.91507, Val: 82.54384, Teach: 0.05, Took 0.1 s\n",
      "Epoch 25 => Train loss: 78.63018, Val: 82.67001, Teach: 0.04, Took 0.1 s\n",
      "Epoch 26 => Train loss: 78.44430, Val: 82.80457, Teach: 0.03, Took 0.1 s\n",
      "Epoch 27 => Train loss: 79.06190, Val: 82.02215, Teach: 0.03, Took 0.1 s      (NEW BEST)\n",
      "Epoch 28 => Train loss: 77.75690, Val: 82.63455, Teach: 0.02, Took 0.1 s\n",
      "Epoch 29 => Train loss: 77.63887, Val: 82.41490, Teach: 0.02, Took 0.1 s\n",
      "Epoch 30 => Train loss: 77.75172, Val: 82.57237, Teach: 0.01, Took 0.1 s\n",
      "Epoch 31 => Train loss: 77.52506, Val: 82.07852, Teach: 0.01, Took 0.1 s\n",
      "Epoch 32 => Train loss: 77.14509, Val: 81.92607, Teach: 0.01, Took 0.1 s      (NEW BEST)\n",
      "Epoch 33 => Train loss: 77.14009, Val: 82.09113, Teach: 0.01, Took 0.1 s\n",
      "Epoch 34 => Train loss: 77.39405, Val: 82.49101, Teach: 0.01, Took 0.1 s\n",
      "Epoch 35 => Train loss: 77.11960, Val: 82.49792, Teach: 0.01, Took 0.1 s\n",
      "Epoch 36 => Train loss: 77.49616, Val: 81.95544, Teach: 0.00, Took 0.1 s\n",
      "Epoch 37 => Train loss: 77.39366, Val: 82.34733, Teach: 0.00, Took 0.1 s\n",
      "Epoch 38 => Train loss: 76.99855, Val: 80.52404, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 39 => Train loss: 77.09822, Val: 81.47248, Teach: 0.00, Took 0.1 s\n",
      "Epoch 40 => Train loss: 76.63551, Val: 80.77137, Teach: 0.00, Took 0.1 s\n",
      "Epoch 41 => Train loss: 76.82485, Val: 81.42772, Teach: 0.00, Took 0.1 s\n",
      "Epoch 42 => Train loss: 77.72085, Val: 80.94602, Teach: 0.00, Took 0.1 s\n",
      "Epoch 43 => Train loss: 76.75104, Val: 80.83634, Teach: 0.00, Took 0.1 s\n",
      "Epoch 44 => Train loss: 76.35950, Val: 80.75569, Teach: 0.00, Took 0.1 s\n",
      "Epoch 45 => Train loss: 76.22362, Val: 80.17266, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 46 => Train loss: 76.06417, Val: 80.38017, Teach: 0.00, Took 0.1 s\n",
      "Epoch 47 => Train loss: 76.37522, Val: 81.17387, Teach: 0.00, Took 0.1 s\n",
      "Epoch 48 => Train loss: 76.20573, Val: 80.93984, Teach: 0.00, Took 0.1 s\n",
      "Epoch 49 => Train loss: 75.92470, Val: 80.14355, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 50 => Train loss: 76.09545, Val: 80.00476, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 51 => Train loss: 75.56574, Val: 80.45760, Teach: 0.00, Took 0.1 s\n",
      "Epoch 52 => Train loss: 75.56126, Val: 79.98317, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 53 => Train loss: 75.37200, Val: 79.78760, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 54 => Train loss: 75.93629, Val: 79.39414, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 55 => Train loss: 75.40760, Val: 79.91473, Teach: 0.00, Took 0.1 s\n",
      "Epoch 56 => Train loss: 75.39972, Val: 80.28852, Teach: 0.00, Took 0.1 s\n",
      "Epoch 57 => Train loss: 74.90661, Val: 79.17915, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 58 => Train loss: 75.74892, Val: 80.06829, Teach: 0.00, Took 0.1 s\n",
      "Epoch 59 => Train loss: 76.19766, Val: 79.94481, Teach: 0.00, Took 0.1 s\n",
      "Epoch 60 => Train loss: 75.09214, Val: 78.82018, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 61 => Train loss: 74.79532, Val: 79.11697, Teach: 0.00, Took 0.1 s\n",
      "Epoch 62 => Train loss: 74.93547, Val: 78.14835, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 63 => Train loss: 75.60976, Val: 78.49090, Teach: 0.00, Took 0.1 s\n",
      "Epoch 64 => Train loss: 75.23732, Val: 79.30522, Teach: 0.00, Took 0.1 s\n",
      "Epoch 65 => Train loss: 74.34015, Val: 78.75373, Teach: 0.00, Took 0.1 s\n",
      "Epoch 66 => Train loss: 74.33517, Val: 78.81326, Teach: 0.00, Took 0.1 s\n",
      "Epoch 67 => Train loss: 74.81717, Val: 78.45502, Teach: 0.00, Took 0.1 s\n",
      "Epoch 68 => Train loss: 74.65939, Val: 78.88642, Teach: 0.00, Took 0.1 s\n",
      "Epoch 69 => Train loss: 74.73118, Val: 78.89601, Teach: 0.00, Took 0.1 s\n",
      "Epoch 70 => Train loss: 74.37289, Val: 78.50483, Teach: 0.00, Took 0.1 s\n",
      "Epoch 71 => Train loss: 74.28987, Val: 78.13528, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 72 => Train loss: 74.09616, Val: 77.79770, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 73 => Train loss: 73.63883, Val: 77.96796, Teach: 0.00, Took 0.1 s\n",
      "Epoch 74 => Train loss: 73.89201, Val: 79.07250, Teach: 0.00, Took 0.1 s\n",
      "Epoch 75 => Train loss: 73.74670, Val: 77.40779, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 76 => Train loss: 73.57406, Val: 77.82490, Teach: 0.00, Took 0.1 s\n",
      "Epoch 77 => Train loss: 74.01997, Val: 77.72415, Teach: 0.00, Took 0.1 s\n",
      "Epoch 78 => Train loss: 73.75760, Val: 77.26144, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 79 => Train loss: 73.35234, Val: 77.02162, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 80 => Train loss: 73.18747, Val: 76.87295, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 81 => Train loss: 73.04874, Val: 76.63221, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 82 => Train loss: 73.02616, Val: 77.26242, Teach: 0.00, Took 0.1 s\n",
      "Epoch 83 => Train loss: 73.81343, Val: 76.81030, Teach: 0.00, Took 0.1 s\n",
      "Epoch 84 => Train loss: 72.92134, Val: 77.78274, Teach: 0.00, Took 0.1 s\n",
      "Epoch 85 => Train loss: 73.51695, Val: 77.63243, Teach: 0.00, Took 0.1 s\n",
      "Epoch 86 => Train loss: 73.13457, Val: 76.61198, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 87 => Train loss: 72.88292, Val: 77.66421, Teach: 0.00, Took 0.1 s\n",
      "Epoch 88 => Train loss: 73.26970, Val: 77.25173, Teach: 0.00, Took 0.1 s\n",
      "Epoch 89 => Train loss: 73.09256, Val: 76.87068, Teach: 0.00, Took 0.1 s\n",
      "Epoch 90 => Train loss: 73.42440, Val: 77.42092, Teach: 0.00, Took 0.1 s\n",
      "Epoch 91 => Train loss: 72.87473, Val: 77.37715, Teach: 0.00, Took 0.1 s\n",
      "Epoch 92 => Train loss: 72.95147, Val: 76.43653, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 93 => Train loss: 72.57278, Val: 75.82221, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 94 => Train loss: 72.77870, Val: 75.99729, Teach: 0.00, Took 0.1 s\n",
      "Epoch 95 => Train loss: 72.87625, Val: 76.91220, Teach: 0.00, Took 0.1 s\n",
      "Epoch 96 => Train loss: 71.86896, Val: 76.78651, Teach: 0.00, Took 0.1 s\n",
      "Epoch 97 => Train loss: 73.36312, Val: 76.60677, Teach: 0.00, Took 0.1 s\n",
      "Epoch 98 => Train loss: 72.35830, Val: 76.59177, Teach: 0.00, Took 0.1 s\n",
      "Epoch 99 => Train loss: 72.85848, Val: 76.57243, Teach: 0.00, Took 0.1 s\n",
      "Epoch 100 => Train loss: 72.02691, Val: 77.07004, Teach: 0.00, Took 0.1 s\n",
      "Epoch 1 => Train loss: 98.80178, Val: 101.66978, Teach: 0.83, Took 0.1 s      (NEW BEST)\n",
      "Epoch 2 => Train loss: 95.97013, Val: 99.12848, Teach: 0.80, Took 0.1 s      (NEW BEST)\n",
      "Epoch 3 => Train loss: 93.23352, Val: 95.78398, Teach: 0.77, Took 0.1 s      (NEW BEST)\n",
      "Epoch 4 => Train loss: 89.79883, Val: 90.87127, Teach: 0.73, Took 0.1 s      (NEW BEST)\n",
      "Epoch 5 => Train loss: 86.43880, Val: 89.51592, Teach: 0.69, Took 0.1 s      (NEW BEST)\n",
      "Epoch 6 => Train loss: 85.43617, Val: 88.37916, Teach: 0.65, Took 0.1 s      (NEW BEST)\n",
      "Epoch 7 => Train loss: 84.58689, Val: 87.93437, Teach: 0.60, Took 0.1 s      (NEW BEST)\n",
      "Epoch 8 => Train loss: 85.16638, Val: 86.83765, Teach: 0.55, Took 0.1 s      (NEW BEST)\n",
      "Epoch 9 => Train loss: 84.22961, Val: 86.43329, Teach: 0.50, Took 0.1 s      (NEW BEST)\n",
      "Epoch 10 => Train loss: 84.50349, Val: 84.88642, Teach: 0.45, Took 0.1 s      (NEW BEST)\n",
      "Epoch 11 => Train loss: 83.30934, Val: 84.91495, Teach: 0.40, Took 0.1 s\n",
      "Epoch 12 => Train loss: 83.32912, Val: 83.08468, Teach: 0.36, Took 0.1 s      (NEW BEST)\n",
      "Epoch 13 => Train loss: 82.10225, Val: 83.97458, Teach: 0.31, Took 0.1 s\n",
      "Epoch 14 => Train loss: 81.60643, Val: 82.10338, Teach: 0.27, Took 0.1 s      (NEW BEST)\n",
      "Epoch 15 => Train loss: 80.27165, Val: 82.15234, Teach: 0.23, Took 0.1 s\n",
      "Epoch 16 => Train loss: 81.04512, Val: 82.05035, Teach: 0.20, Took 0.1 s      (NEW BEST)\n",
      "Epoch 17 => Train loss: 80.39052, Val: 80.54937, Teach: 0.17, Took 0.1 s      (NEW BEST)\n",
      "Epoch 18 => Train loss: 79.66161, Val: 79.49581, Teach: 0.14, Took 0.1 s      (NEW BEST)\n",
      "Epoch 19 => Train loss: 78.46312, Val: 79.64302, Teach: 0.12, Took 0.1 s\n",
      "Epoch 20 => Train loss: 79.65760, Val: 78.49427, Teach: 0.10, Took 0.1 s      (NEW BEST)\n",
      "Epoch 21 => Train loss: 78.71659, Val: 78.03979, Teach: 0.08, Took 0.1 s      (NEW BEST)\n",
      "Epoch 22 => Train loss: 78.16699, Val: 78.38516, Teach: 0.07, Took 0.1 s\n",
      "Epoch 23 => Train loss: 77.88069, Val: 78.34182, Teach: 0.06, Took 0.1 s\n",
      "Epoch 24 => Train loss: 77.01008, Val: 77.70955, Teach: 0.05, Took 0.1 s      (NEW BEST)\n",
      "Epoch 25 => Train loss: 76.98915, Val: 76.96585, Teach: 0.04, Took 0.1 s      (NEW BEST)\n",
      "Epoch 26 => Train loss: 77.66502, Val: 77.18111, Teach: 0.03, Took 0.1 s\n",
      "Epoch 27 => Train loss: 76.32900, Val: 76.73964, Teach: 0.03, Took 0.1 s      (NEW BEST)\n",
      "Epoch 28 => Train loss: 76.74124, Val: 76.71960, Teach: 0.02, Took 0.1 s      (NEW BEST)\n",
      "Epoch 29 => Train loss: 76.84652, Val: 75.65131, Teach: 0.02, Took 0.1 s      (NEW BEST)\n",
      "Epoch 30 => Train loss: 75.58973, Val: 75.72805, Teach: 0.01, Took 0.1 s\n",
      "Epoch 31 => Train loss: 75.92938, Val: 76.09546, Teach: 0.01, Took 0.1 s\n",
      "Epoch 32 => Train loss: 76.19696, Val: 76.05086, Teach: 0.01, Took 0.1 s\n",
      "Epoch 33 => Train loss: 75.97704, Val: 76.82038, Teach: 0.01, Took 0.1 s\n",
      "Epoch 34 => Train loss: 75.53192, Val: 75.82427, Teach: 0.01, Took 0.1 s\n",
      "Epoch 35 => Train loss: 75.77367, Val: 75.86283, Teach: 0.01, Took 0.1 s\n",
      "Epoch 36 => Train loss: 75.50001, Val: 76.20738, Teach: 0.00, Took 0.1 s\n",
      "Epoch 37 => Train loss: 74.92340, Val: 75.24671, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 38 => Train loss: 74.88330, Val: 75.97341, Teach: 0.00, Took 0.1 s\n",
      "Epoch 39 => Train loss: 74.90174, Val: 75.97846, Teach: 0.00, Took 0.1 s\n",
      "Epoch 40 => Train loss: 75.54708, Val: 75.16996, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 41 => Train loss: 75.81063, Val: 75.10296, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 42 => Train loss: 75.29784, Val: 75.04944, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 43 => Train loss: 75.33647, Val: 75.38253, Teach: 0.00, Took 0.1 s\n",
      "Epoch 44 => Train loss: 75.22917, Val: 75.62683, Teach: 0.00, Took 0.1 s\n",
      "Epoch 45 => Train loss: 75.23319, Val: 75.89865, Teach: 0.00, Took 0.1 s\n",
      "Epoch 46 => Train loss: 75.45334, Val: 76.08524, Teach: 0.00, Took 0.1 s\n",
      "Epoch 47 => Train loss: 75.80770, Val: 74.39817, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 48 => Train loss: 74.69192, Val: 76.30174, Teach: 0.00, Took 0.1 s\n",
      "Epoch 49 => Train loss: 74.25000, Val: 75.29667, Teach: 0.00, Took 0.1 s\n",
      "Epoch 50 => Train loss: 75.73368, Val: 74.35441, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 51 => Train loss: 74.83884, Val: 74.25378, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 52 => Train loss: 74.55672, Val: 74.77920, Teach: 0.00, Took 0.1 s\n",
      "Epoch 53 => Train loss: 75.02639, Val: 74.45796, Teach: 0.00, Took 0.1 s\n",
      "Epoch 54 => Train loss: 74.48733, Val: 74.24104, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 55 => Train loss: 75.05836, Val: 75.18513, Teach: 0.00, Took 0.1 s\n",
      "Epoch 56 => Train loss: 74.74934, Val: 74.98663, Teach: 0.00, Took 0.1 s\n",
      "Epoch 57 => Train loss: 74.62290, Val: 74.75793, Teach: 0.00, Took 0.1 s\n",
      "Epoch 58 => Train loss: 74.06492, Val: 74.67485, Teach: 0.00, Took 0.1 s\n",
      "Epoch 59 => Train loss: 74.97440, Val: 75.70243, Teach: 0.00, Took 0.1 s\n",
      "Epoch 60 => Train loss: 74.39420, Val: 73.92053, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 61 => Train loss: 74.99829, Val: 73.28523, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 62 => Train loss: 74.61914, Val: 73.57491, Teach: 0.00, Took 0.1 s\n",
      "Epoch 63 => Train loss: 74.18784, Val: 73.83720, Teach: 0.00, Took 0.1 s\n",
      "Epoch 64 => Train loss: 74.41111, Val: 74.50785, Teach: 0.00, Took 0.1 s\n",
      "Epoch 65 => Train loss: 74.50134, Val: 74.40622, Teach: 0.00, Took 0.1 s\n",
      "Epoch 66 => Train loss: 74.73629, Val: 74.68818, Teach: 0.00, Took 0.1 s\n",
      "Epoch 67 => Train loss: 73.66186, Val: 74.35919, Teach: 0.00, Took 0.1 s\n",
      "Epoch 68 => Train loss: 74.07369, Val: 73.55777, Teach: 0.00, Took 0.1 s\n",
      "Epoch 69 => Train loss: 74.00566, Val: 74.43904, Teach: 0.00, Took 0.1 s\n",
      "Epoch 70 => Train loss: 74.49695, Val: 73.66600, Teach: 0.00, Took 0.1 s\n",
      "Epoch 71 => Train loss: 74.08723, Val: 74.95757, Teach: 0.00, Took 0.1 s\n",
      "Epoch 72 => Train loss: 73.22794, Val: 74.23514, Teach: 0.00, Took 0.1 s\n",
      "Epoch 73 => Train loss: 74.78114, Val: 75.13372, Teach: 0.00, Took 0.1 s\n",
      "Epoch 74 => Train loss: 73.92854, Val: 74.42407, Teach: 0.00, Took 0.1 s\n",
      "Epoch 75 => Train loss: 74.34230, Val: 74.06839, Teach: 0.00, Took 0.1 s\n",
      "Epoch 76 => Train loss: 73.79858, Val: 74.01716, Teach: 0.00, Took 0.1 s\n",
      "Epoch 77 => Train loss: 73.76692, Val: 74.11810, Teach: 0.00, Took 0.1 s\n",
      "Epoch 78 => Train loss: 73.96919, Val: 73.68381, Teach: 0.00, Took 0.1 s\n",
      "Epoch 79 => Train loss: 74.64956, Val: 74.18468, Teach: 0.00, Took 0.1 s\n",
      "Epoch 80 => Train loss: 73.79775, Val: 74.57502, Teach: 0.00, Took 0.1 s\n",
      "Epoch 81 => Train loss: 73.76472, Val: 74.10984, Teach: 0.00, Took 0.1 s\n",
      "Epoch 82 => Train loss: 73.92245, Val: 74.60741, Teach: 0.00, Took 0.1 s\n",
      "Epoch 83 => Train loss: 74.30702, Val: 75.15430, Teach: 0.00, Took 0.1 s\n",
      "Epoch 84 => Train loss: 73.68105, Val: 73.12417, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 85 => Train loss: 74.10754, Val: 73.91737, Teach: 0.00, Took 0.1 s\n",
      "Epoch 86 => Train loss: 73.97198, Val: 73.94659, Teach: 0.00, Took 0.1 s\n",
      "Epoch 87 => Train loss: 73.51341, Val: 72.33039, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 88 => Train loss: 73.64744, Val: 74.07481, Teach: 0.00, Took 0.1 s\n",
      "Epoch 89 => Train loss: 74.04938, Val: 73.22399, Teach: 0.00, Took 0.1 s\n",
      "Epoch 90 => Train loss: 73.91113, Val: 74.56040, Teach: 0.00, Took 0.1 s\n",
      "Epoch 91 => Train loss: 73.20395, Val: 73.33039, Teach: 0.00, Took 0.1 s\n",
      "Epoch 92 => Train loss: 74.07955, Val: 73.94539, Teach: 0.00, Took 0.1 s\n",
      "Epoch 93 => Train loss: 74.44768, Val: 73.67805, Teach: 0.00, Took 0.1 s\n",
      "Epoch 94 => Train loss: 74.06639, Val: 73.25698, Teach: 0.00, Took 0.1 s\n",
      "Epoch 95 => Train loss: 73.37418, Val: 73.70132, Teach: 0.00, Took 0.1 s\n",
      "Epoch 96 => Train loss: 73.91240, Val: 73.62290, Teach: 0.00, Took 0.1 s\n",
      "Epoch 97 => Train loss: 74.49036, Val: 73.00162, Teach: 0.00, Took 0.1 s\n",
      "Epoch 98 => Train loss: 73.68997, Val: 73.61107, Teach: 0.00, Took 0.1 s\n",
      "Epoch 99 => Train loss: 73.75741, Val: 72.60446, Teach: 0.00, Took 0.1 s\n",
      "Epoch 100 => Train loss: 73.58158, Val: 73.76146, Teach: 0.00, Took 0.1 s\n",
      "Epoch 1 => Train loss: 149.35835, Val: 150.12990, Teach: 0.83, Took 0.1 s      (NEW BEST)\n",
      "Epoch 2 => Train loss: 139.68416, Val: 143.93072, Teach: 0.80, Took 0.1 s      (NEW BEST)\n",
      "Epoch 3 => Train loss: 133.88702, Val: 138.83557, Teach: 0.77, Took 0.1 s      (NEW BEST)\n",
      "Epoch 4 => Train loss: 127.90995, Val: 128.71587, Teach: 0.73, Took 0.1 s      (NEW BEST)\n",
      "Epoch 5 => Train loss: 118.03770, Val: 116.38198, Teach: 0.69, Took 0.1 s      (NEW BEST)\n",
      "Epoch 6 => Train loss: 106.36861, Val: 103.04792, Teach: 0.65, Took 0.1 s      (NEW BEST)\n",
      "Epoch 7 => Train loss: 94.85426, Val: 96.33252, Teach: 0.60, Took 0.1 s      (NEW BEST)\n",
      "Epoch 8 => Train loss: 90.11901, Val: 95.75833, Teach: 0.55, Took 0.1 s      (NEW BEST)\n",
      "Epoch 9 => Train loss: 87.94426, Val: 95.41275, Teach: 0.50, Took 0.1 s      (NEW BEST)\n",
      "Epoch 10 => Train loss: 86.61057, Val: 92.29933, Teach: 0.45, Took 0.1 s      (NEW BEST)\n",
      "Epoch 11 => Train loss: 85.83523, Val: 92.70283, Teach: 0.40, Took 0.1 s\n",
      "Epoch 12 => Train loss: 84.74662, Val: 93.55546, Teach: 0.36, Took 0.1 s\n",
      "Epoch 13 => Train loss: 84.06704, Val: 90.63265, Teach: 0.31, Took 0.1 s      (NEW BEST)\n",
      "Epoch 14 => Train loss: 82.19863, Val: 90.61253, Teach: 0.27, Took 0.1 s      (NEW BEST)\n",
      "Epoch 15 => Train loss: 81.27758, Val: 89.09596, Teach: 0.23, Took 0.1 s      (NEW BEST)\n",
      "Epoch 16 => Train loss: 81.22412, Val: 88.39835, Teach: 0.20, Took 0.1 s      (NEW BEST)\n",
      "Epoch 17 => Train loss: 80.41794, Val: 87.90977, Teach: 0.17, Took 0.1 s      (NEW BEST)\n",
      "Epoch 18 => Train loss: 79.81554, Val: 87.52748, Teach: 0.14, Took 0.1 s      (NEW BEST)\n",
      "Epoch 19 => Train loss: 79.53963, Val: 86.71495, Teach: 0.12, Took 0.1 s      (NEW BEST)\n",
      "Epoch 20 => Train loss: 78.46349, Val: 86.76215, Teach: 0.10, Took 0.1 s\n",
      "Epoch 21 => Train loss: 78.38007, Val: 86.14787, Teach: 0.08, Took 0.1 s      (NEW BEST)\n",
      "Epoch 22 => Train loss: 77.54350, Val: 85.63532, Teach: 0.07, Took 0.1 s      (NEW BEST)\n",
      "Epoch 23 => Train loss: 77.43463, Val: 85.89315, Teach: 0.06, Took 0.1 s\n",
      "Epoch 24 => Train loss: 76.95039, Val: 84.18284, Teach: 0.05, Took 0.1 s      (NEW BEST)\n",
      "Epoch 25 => Train loss: 76.93252, Val: 83.98779, Teach: 0.04, Took 0.1 s      (NEW BEST)\n",
      "Epoch 26 => Train loss: 76.89801, Val: 84.07220, Teach: 0.03, Took 0.1 s\n",
      "Epoch 27 => Train loss: 76.00449, Val: 83.45928, Teach: 0.03, Took 0.2 s      (NEW BEST)\n",
      "Epoch 28 => Train loss: 76.42736, Val: 84.04072, Teach: 0.02, Took 0.1 s\n",
      "Epoch 29 => Train loss: 76.21653, Val: 83.15234, Teach: 0.02, Took 0.1 s      (NEW BEST)\n",
      "Epoch 30 => Train loss: 75.30749, Val: 83.00742, Teach: 0.01, Took 0.1 s      (NEW BEST)\n",
      "Epoch 31 => Train loss: 74.29715, Val: 81.44775, Teach: 0.01, Took 0.1 s      (NEW BEST)\n",
      "Epoch 32 => Train loss: 75.01785, Val: 81.32599, Teach: 0.01, Took 0.1 s      (NEW BEST)\n",
      "Epoch 33 => Train loss: 74.87842, Val: 81.79390, Teach: 0.01, Took 0.1 s\n",
      "Epoch 34 => Train loss: 74.35100, Val: 80.92584, Teach: 0.01, Took 0.1 s      (NEW BEST)\n",
      "Epoch 35 => Train loss: 74.90683, Val: 81.05244, Teach: 0.01, Took 0.1 s\n",
      "Epoch 36 => Train loss: 74.20036, Val: 80.39742, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 37 => Train loss: 74.47076, Val: 81.06172, Teach: 0.00, Took 0.1 s\n",
      "Epoch 38 => Train loss: 74.22898, Val: 80.42763, Teach: 0.00, Took 0.1 s\n",
      "Epoch 39 => Train loss: 74.02194, Val: 80.71464, Teach: 0.00, Took 0.1 s\n",
      "Epoch 40 => Train loss: 73.11496, Val: 80.86334, Teach: 0.00, Took 0.1 s\n",
      "Epoch 41 => Train loss: 73.81524, Val: 79.30272, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 42 => Train loss: 73.81957, Val: 80.94592, Teach: 0.00, Took 0.1 s\n",
      "Epoch 43 => Train loss: 73.69668, Val: 80.90064, Teach: 0.00, Took 0.1 s\n",
      "Epoch 44 => Train loss: 73.92899, Val: 80.56656, Teach: 0.00, Took 0.1 s\n",
      "Epoch 45 => Train loss: 73.72980, Val: 80.01657, Teach: 0.00, Took 0.1 s\n",
      "Epoch 46 => Train loss: 73.22873, Val: 79.69279, Teach: 0.00, Took 0.1 s\n",
      "Epoch 47 => Train loss: 73.52978, Val: 80.46101, Teach: 0.00, Took 0.1 s\n",
      "Epoch 48 => Train loss: 73.59303, Val: 79.93503, Teach: 0.00, Took 0.1 s\n",
      "Epoch 49 => Train loss: 73.26951, Val: 80.15814, Teach: 0.00, Took 0.1 s\n",
      "Epoch 50 => Train loss: 72.95407, Val: 80.74763, Teach: 0.00, Took 0.1 s\n",
      "Epoch 51 => Train loss: 73.60474, Val: 79.55894, Teach: 0.00, Took 0.1 s\n",
      "Epoch 52 => Train loss: 73.31416, Val: 79.48711, Teach: 0.00, Took 0.1 s\n",
      "Epoch 53 => Train loss: 73.25765, Val: 79.44346, Teach: 0.00, Took 0.1 s\n",
      "Epoch 54 => Train loss: 72.79451, Val: 78.88863, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 55 => Train loss: 73.08301, Val: 81.16186, Teach: 0.00, Took 0.1 s\n",
      "Epoch 56 => Train loss: 73.07599, Val: 80.29540, Teach: 0.00, Took 0.1 s\n",
      "Epoch 57 => Train loss: 73.62407, Val: 78.79121, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 58 => Train loss: 72.57642, Val: 79.72961, Teach: 0.00, Took 0.1 s\n",
      "Epoch 59 => Train loss: 72.85225, Val: 79.26012, Teach: 0.00, Took 0.1 s\n",
      "Epoch 60 => Train loss: 73.42175, Val: 79.07339, Teach: 0.00, Took 0.1 s\n",
      "Epoch 61 => Train loss: 72.74914, Val: 80.05234, Teach: 0.00, Took 0.1 s\n",
      "Epoch 62 => Train loss: 73.51046, Val: 79.76455, Teach: 0.00, Took 0.1 s\n",
      "Epoch 63 => Train loss: 72.83759, Val: 79.13642, Teach: 0.00, Took 0.1 s\n",
      "Epoch 64 => Train loss: 73.09742, Val: 80.43763, Teach: 0.00, Took 0.1 s\n",
      "Epoch 65 => Train loss: 73.03678, Val: 79.54719, Teach: 0.00, Took 0.1 s\n",
      "Epoch 66 => Train loss: 72.44743, Val: 78.94731, Teach: 0.00, Took 0.1 s\n",
      "Epoch 67 => Train loss: 72.88128, Val: 78.91138, Teach: 0.00, Took 0.1 s\n",
      "Epoch 68 => Train loss: 73.05291, Val: 78.02957, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 69 => Train loss: 72.81375, Val: 79.03036, Teach: 0.00, Took 0.1 s\n",
      "Epoch 70 => Train loss: 72.45038, Val: 79.89749, Teach: 0.00, Took 0.1 s\n",
      "Epoch 71 => Train loss: 72.54148, Val: 79.34698, Teach: 0.00, Took 0.1 s\n",
      "Epoch 72 => Train loss: 72.03026, Val: 80.28279, Teach: 0.00, Took 0.1 s\n",
      "Epoch 73 => Train loss: 73.37796, Val: 78.92204, Teach: 0.00, Took 0.1 s\n",
      "Epoch 74 => Train loss: 72.52524, Val: 79.56312, Teach: 0.00, Took 0.1 s\n",
      "Epoch 75 => Train loss: 72.23917, Val: 79.42060, Teach: 0.00, Took 0.1 s\n",
      "Epoch 76 => Train loss: 71.98956, Val: 78.95673, Teach: 0.00, Took 0.1 s\n",
      "Epoch 77 => Train loss: 72.26336, Val: 79.02085, Teach: 0.00, Took 0.1 s\n",
      "Epoch 78 => Train loss: 72.82897, Val: 79.55917, Teach: 0.00, Took 0.1 s\n",
      "Epoch 79 => Train loss: 72.51987, Val: 77.71741, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 80 => Train loss: 72.12484, Val: 79.24957, Teach: 0.00, Took 0.1 s\n",
      "Epoch 81 => Train loss: 73.67011, Val: 79.58206, Teach: 0.00, Took 0.1 s\n",
      "Epoch 82 => Train loss: 72.80350, Val: 79.47002, Teach: 0.00, Took 0.1 s\n",
      "Epoch 83 => Train loss: 72.17566, Val: 77.91030, Teach: 0.00, Took 0.1 s\n",
      "Epoch 84 => Train loss: 72.54551, Val: 78.51034, Teach: 0.00, Took 0.1 s\n",
      "Epoch 85 => Train loss: 72.24893, Val: 78.23919, Teach: 0.00, Took 0.1 s\n",
      "Epoch 86 => Train loss: 72.52507, Val: 77.96586, Teach: 0.00, Took 0.1 s\n",
      "Epoch 87 => Train loss: 72.86340, Val: 78.03067, Teach: 0.00, Took 0.1 s\n",
      "Epoch 88 => Train loss: 72.20666, Val: 78.43511, Teach: 0.00, Took 0.1 s\n",
      "Epoch 89 => Train loss: 73.14674, Val: 78.60083, Teach: 0.00, Took 0.1 s\n",
      "Epoch 90 => Train loss: 72.14695, Val: 79.01643, Teach: 0.00, Took 0.1 s\n",
      "Epoch 91 => Train loss: 72.41909, Val: 78.34096, Teach: 0.00, Took 0.1 s\n",
      "Epoch 92 => Train loss: 72.43892, Val: 78.31736, Teach: 0.00, Took 0.1 s\n",
      "Epoch 93 => Train loss: 71.89512, Val: 79.28594, Teach: 0.00, Took 0.1 s\n",
      "Epoch 94 => Train loss: 71.80423, Val: 79.16130, Teach: 0.00, Took 0.1 s\n",
      "Epoch 95 => Train loss: 72.47872, Val: 79.43184, Teach: 0.00, Took 0.1 s\n",
      "Epoch 96 => Train loss: 72.56418, Val: 78.06338, Teach: 0.00, Took 0.1 s\n",
      "Epoch 97 => Train loss: 72.74533, Val: 78.44955, Teach: 0.00, Took 0.1 s\n",
      "Epoch 98 => Train loss: 72.42150, Val: 79.04170, Teach: 0.00, Took 0.1 s\n",
      "Epoch 99 => Train loss: 71.81803, Val: 78.73561, Teach: 0.00, Took 0.1 s\n",
      "Epoch 100 => Train loss: 72.73536, Val: 78.08093, Teach: 0.00, Took 0.1 s\n",
      "Epoch 1 => Train loss: 172.26836, Val: 170.53566, Teach: 0.83, Took 0.1 s      (NEW BEST)\n",
      "Epoch 2 => Train loss: 167.16874, Val: 160.60675, Teach: 0.80, Took 0.1 s      (NEW BEST)\n",
      "Epoch 3 => Train loss: 157.87533, Val: 148.87909, Teach: 0.77, Took 0.1 s      (NEW BEST)\n",
      "Epoch 4 => Train loss: 142.62114, Val: 138.48142, Teach: 0.73, Took 0.1 s      (NEW BEST)\n",
      "Epoch 5 => Train loss: 131.56847, Val: 127.13220, Teach: 0.69, Took 0.1 s      (NEW BEST)\n",
      "Epoch 6 => Train loss: 119.41187, Val: 113.58920, Teach: 0.65, Took 0.1 s      (NEW BEST)\n",
      "Epoch 7 => Train loss: 107.06218, Val: 104.15797, Teach: 0.60, Took 0.1 s      (NEW BEST)\n",
      "Epoch 8 => Train loss: 100.68162, Val: 101.14881, Teach: 0.55, Took 0.1 s      (NEW BEST)\n",
      "Epoch 9 => Train loss: 98.06850, Val: 98.94635, Teach: 0.50, Took 0.1 s      (NEW BEST)\n",
      "Epoch 10 => Train loss: 97.63710, Val: 97.92212, Teach: 0.45, Took 0.1 s      (NEW BEST)\n",
      "Epoch 11 => Train loss: 97.03965, Val: 97.51815, Teach: 0.40, Took 0.1 s      (NEW BEST)\n",
      "Epoch 12 => Train loss: 95.59403, Val: 96.60045, Teach: 0.36, Took 0.1 s      (NEW BEST)\n",
      "Epoch 13 => Train loss: 95.08755, Val: 95.45386, Teach: 0.31, Took 0.1 s      (NEW BEST)\n",
      "Epoch 14 => Train loss: 93.89592, Val: 94.47903, Teach: 0.27, Took 0.1 s      (NEW BEST)\n",
      "Epoch 15 => Train loss: 92.86057, Val: 95.20262, Teach: 0.23, Took 0.1 s\n",
      "Epoch 16 => Train loss: 92.91613, Val: 93.59674, Teach: 0.20, Took 0.1 s      (NEW BEST)\n",
      "Epoch 17 => Train loss: 92.67017, Val: 92.98910, Teach: 0.17, Took 0.1 s      (NEW BEST)\n",
      "Epoch 18 => Train loss: 91.66358, Val: 92.31494, Teach: 0.14, Took 0.1 s      (NEW BEST)\n",
      "Epoch 19 => Train loss: 91.09675, Val: 91.86554, Teach: 0.12, Took 0.1 s      (NEW BEST)\n",
      "Epoch 20 => Train loss: 90.81548, Val: 92.33955, Teach: 0.10, Took 0.1 s\n",
      "Epoch 21 => Train loss: 90.25414, Val: 91.57676, Teach: 0.08, Took 0.1 s      (NEW BEST)\n",
      "Epoch 22 => Train loss: 89.85588, Val: 91.68174, Teach: 0.07, Took 0.1 s\n",
      "Epoch 23 => Train loss: 89.49725, Val: 91.32722, Teach: 0.06, Took 0.1 s      (NEW BEST)\n",
      "Epoch 24 => Train loss: 89.73003, Val: 91.67832, Teach: 0.05, Took 0.1 s\n",
      "Epoch 25 => Train loss: 89.71629, Val: 90.24410, Teach: 0.04, Took 0.1 s      (NEW BEST)\n",
      "Epoch 26 => Train loss: 89.13934, Val: 89.85850, Teach: 0.03, Took 0.1 s      (NEW BEST)\n",
      "Epoch 27 => Train loss: 87.23559, Val: 86.56605, Teach: 0.03, Took 0.1 s      (NEW BEST)\n",
      "Epoch 28 => Train loss: 82.04059, Val: 82.83732, Teach: 0.02, Took 0.1 s      (NEW BEST)\n",
      "Epoch 29 => Train loss: 78.79267, Val: 80.88509, Teach: 0.02, Took 0.1 s      (NEW BEST)\n",
      "Epoch 30 => Train loss: 78.65988, Val: 80.53148, Teach: 0.01, Took 0.1 s      (NEW BEST)\n",
      "Epoch 31 => Train loss: 77.86592, Val: 79.82307, Teach: 0.01, Took 0.1 s      (NEW BEST)\n",
      "Epoch 32 => Train loss: 77.31237, Val: 80.84032, Teach: 0.01, Took 0.1 s\n",
      "Epoch 33 => Train loss: 76.90145, Val: 79.79492, Teach: 0.01, Took 0.1 s      (NEW BEST)\n",
      "Epoch 34 => Train loss: 76.79528, Val: 79.88020, Teach: 0.01, Took 0.1 s\n",
      "Epoch 35 => Train loss: 77.36167, Val: 78.87949, Teach: 0.01, Took 0.1 s      (NEW BEST)\n",
      "Epoch 36 => Train loss: 77.68822, Val: 80.10950, Teach: 0.00, Took 0.1 s\n",
      "Epoch 37 => Train loss: 76.88887, Val: 79.41416, Teach: 0.00, Took 0.1 s\n",
      "Epoch 38 => Train loss: 76.75433, Val: 78.54715, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 39 => Train loss: 76.09434, Val: 79.42042, Teach: 0.00, Took 0.1 s\n",
      "Epoch 40 => Train loss: 76.08778, Val: 78.71383, Teach: 0.00, Took 0.1 s\n",
      "Epoch 41 => Train loss: 76.14880, Val: 78.97810, Teach: 0.00, Took 0.1 s\n",
      "Epoch 42 => Train loss: 76.06668, Val: 78.64765, Teach: 0.00, Took 0.1 s\n",
      "Epoch 43 => Train loss: 76.07738, Val: 77.23145, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 44 => Train loss: 75.67796, Val: 77.94549, Teach: 0.00, Took 0.1 s\n",
      "Epoch 45 => Train loss: 75.72172, Val: 77.11037, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 46 => Train loss: 75.89954, Val: 77.27603, Teach: 0.00, Took 0.1 s\n",
      "Epoch 47 => Train loss: 75.76776, Val: 77.63256, Teach: 0.00, Took 0.1 s\n",
      "Epoch 48 => Train loss: 75.76718, Val: 77.07170, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 49 => Train loss: 75.37151, Val: 76.91850, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 50 => Train loss: 74.26510, Val: 76.99138, Teach: 0.00, Took 0.1 s\n",
      "Epoch 51 => Train loss: 74.41121, Val: 77.30130, Teach: 0.00, Took 0.1 s\n",
      "Epoch 52 => Train loss: 75.39436, Val: 76.34643, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 53 => Train loss: 74.48141, Val: 77.14173, Teach: 0.00, Took 0.1 s\n",
      "Epoch 54 => Train loss: 74.42180, Val: 76.73518, Teach: 0.00, Took 0.1 s\n",
      "Epoch 55 => Train loss: 74.97027, Val: 75.77829, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 56 => Train loss: 73.90102, Val: 77.42908, Teach: 0.00, Took 0.1 s\n",
      "Epoch 57 => Train loss: 74.25922, Val: 76.32555, Teach: 0.00, Took 0.1 s\n",
      "Epoch 58 => Train loss: 73.85197, Val: 76.09344, Teach: 0.00, Took 0.1 s\n",
      "Epoch 59 => Train loss: 73.95855, Val: 75.42148, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 60 => Train loss: 74.11831, Val: 75.23577, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 61 => Train loss: 73.54843, Val: 75.87721, Teach: 0.00, Took 0.1 s\n",
      "Epoch 62 => Train loss: 73.34213, Val: 75.87706, Teach: 0.00, Took 0.1 s\n",
      "Epoch 63 => Train loss: 73.69426, Val: 76.19147, Teach: 0.00, Took 0.1 s\n",
      "Epoch 64 => Train loss: 73.64007, Val: 76.07876, Teach: 0.00, Took 0.1 s\n",
      "Epoch 65 => Train loss: 73.32432, Val: 76.71006, Teach: 0.00, Took 0.1 s\n",
      "Epoch 66 => Train loss: 73.68007, Val: 75.50512, Teach: 0.00, Took 0.1 s\n",
      "Epoch 67 => Train loss: 72.79437, Val: 75.12582, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 68 => Train loss: 73.30196, Val: 75.75597, Teach: 0.00, Took 0.1 s\n",
      "Epoch 69 => Train loss: 73.81675, Val: 75.88030, Teach: 0.00, Took 0.1 s\n",
      "Epoch 70 => Train loss: 72.70591, Val: 76.79673, Teach: 0.00, Took 0.1 s\n",
      "Epoch 71 => Train loss: 73.44975, Val: 75.52804, Teach: 0.00, Took 0.1 s\n",
      "Epoch 72 => Train loss: 73.17359, Val: 76.13941, Teach: 0.00, Took 0.1 s\n",
      "Epoch 73 => Train loss: 73.15310, Val: 74.59892, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 74 => Train loss: 73.12963, Val: 75.75675, Teach: 0.00, Took 0.1 s\n",
      "Epoch 75 => Train loss: 72.82524, Val: 75.41742, Teach: 0.00, Took 0.1 s\n",
      "Epoch 76 => Train loss: 73.15743, Val: 75.47270, Teach: 0.00, Took 0.1 s\n",
      "Epoch 77 => Train loss: 73.71437, Val: 75.11342, Teach: 0.00, Took 0.1 s\n",
      "Epoch 78 => Train loss: 73.44023, Val: 75.13014, Teach: 0.00, Took 0.1 s\n",
      "Epoch 79 => Train loss: 73.36361, Val: 75.25708, Teach: 0.00, Took 0.1 s\n",
      "Epoch 80 => Train loss: 73.31028, Val: 75.54538, Teach: 0.00, Took 0.1 s\n",
      "Epoch 81 => Train loss: 72.77832, Val: 75.54871, Teach: 0.00, Took 0.1 s\n",
      "Epoch 82 => Train loss: 73.77985, Val: 74.80595, Teach: 0.00, Took 0.1 s\n",
      "Epoch 83 => Train loss: 73.83600, Val: 74.94615, Teach: 0.00, Took 0.1 s\n",
      "Epoch 84 => Train loss: 74.04152, Val: 75.28602, Teach: 0.00, Took 0.1 s\n",
      "Epoch 85 => Train loss: 73.04479, Val: 76.27785, Teach: 0.00, Took 0.1 s\n",
      "Epoch 86 => Train loss: 73.62596, Val: 75.96255, Teach: 0.00, Took 0.1 s\n",
      "Epoch 87 => Train loss: 73.33086, Val: 75.29219, Teach: 0.00, Took 0.1 s\n",
      "Epoch 88 => Train loss: 73.01037, Val: 75.18757, Teach: 0.00, Took 0.1 s\n",
      "Epoch 89 => Train loss: 73.76787, Val: 74.86860, Teach: 0.00, Took 0.1 s\n",
      "Epoch 90 => Train loss: 73.19533, Val: 75.69867, Teach: 0.00, Took 0.1 s\n",
      "Epoch 91 => Train loss: 73.49952, Val: 74.57520, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 92 => Train loss: 73.73113, Val: 75.64984, Teach: 0.00, Took 0.1 s\n",
      "Epoch 93 => Train loss: 73.37973, Val: 74.16548, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 94 => Train loss: 73.20844, Val: 74.11382, Teach: 0.00, Took 0.1 s      (NEW BEST)\n",
      "Epoch 95 => Train loss: 72.85199, Val: 74.92447, Teach: 0.00, Took 0.1 s\n",
      "Epoch 96 => Train loss: 72.96905, Val: 75.14589, Teach: 0.00, Took 0.1 s\n",
      "Epoch 97 => Train loss: 72.58501, Val: 76.04256, Teach: 0.00, Took 0.1 s\n",
      "Epoch 98 => Train loss: 73.33782, Val: 74.42288, Teach: 0.00, Took 0.1 s\n",
      "Epoch 99 => Train loss: 72.73671, Val: 75.71288, Teach: 0.00, Took 0.1 s\n",
      "Epoch 100 => Train loss: 73.01591, Val: 74.89841, Teach: 0.00, Took 0.1 s\n"
     ]
    }
   ],
   "source": [
    "from aux import get_best_model\n",
    "from aux import evaluate\n",
    "import json\n",
    "results = []\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    train_data, val_data = train_val_split(data, p = 0.8) # 80% of data to train\n",
    "    encoder = Encoder(enc_feature_size, hidden_size, num_layers, dropout)\n",
    "    decoder_args = (dec_feature_size, dec_target_size, hidden_size, num_layers, target_indices, dropout, device)\n",
    "    decoder = DecoderWithAttention(*decoder_args)\n",
    "    seq2seq = Seq2Seq(encoder, decoder, lr, grad_clip).to(device)\n",
    "    best_model = get_best_model(seq2seq, train_data, val_data, batch_size, num_epochs, decay)\n",
    "    results.append(evaluate(best_model, val_data, batch_size))\n",
    "with open(\"../../results.txt\", \"a\", encoding = \"utf8\") as f:\n",
    "    json.dump(\n",
    "        {k:v for k,v in locals().items()\n",
    "            if k in {\"hidden_size\", \"num_layers\", \"dropout\", \"lr\", \"grad_clip\", \"batch_size\", \"num_epochs\", \"decay\", \"input_feautures_name\", \"rnn_type\", \"n\"}} | {\n",
    "                \"val_mean\" : float(np.mean(results)), \"val_std\" : float(np.std(results))},\n",
    "        f\n",
    "    )\n",
    "    f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a158ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot import plot\n",
    "plot(i,best_model, val_data, max_month)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.savefig(f\"seq2seq-model-results-{i}.png\")\n",
    "i+=1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parkinsons",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
